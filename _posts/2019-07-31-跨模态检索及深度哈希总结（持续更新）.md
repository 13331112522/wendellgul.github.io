---
title: 跨模态检索及深度哈希总结（持续更新）
category: Research Note
date: 2019-07-31
tag:
 - cross-modal retrieval
 - deep hashing
 - image retrieval
---

跨模态检索是指在不同模态数据之间的检索，比如通过一张图片检索与之相关的文本、音视频等数据，或者通过一段文本检索与之相关的图片等数据，解决跨模态检索的主要方法有两种，一种是学习不同模态数据的**实值表示**，通过距离度量（如余弦距离、欧氏距离等）进行相关度排序的方法；另一种是学习不同模态数据的**二值哈希码**，通过度量汉明距离进行相关度排序的方法，这种方法又称为**跨模态哈希**，由于汉明距离计算的高效性，跨模态哈希在大规模跨模态检索中运用十分广泛，而利用深度学习进行的跨模态哈希，即为深度跨模态哈希。

本文是对跨模态检索和深度哈希阶段性总结，主要针对自2016年来的相关工作。

<!-- more -->

### 跨模态检索

#### DCMH (CVPR 2017)

论文：[Deep Cross-Modal Hashing][1]。DCMH作为最早的（在DVSH之前上传于arXiv上）基于深度学习的跨模态哈希方法，是南京大学LAMDA实验室李武军老师团队的工作。DCMH将跨模态哈希任务分为了两个部分，特征学习部分和哈希码学习部分，这也奠定了之后一些工作的网络结构。

<center><img src="https://wendellgul.github.io/assets/images/Deep%20Cross-Modal%20Hashing/1533709036203.png" /></center>
图片经过 AlexNet 或其他卷积神经网络得到实值哈希码，文本经过多层感知机得到实值哈希码，哈希码学习部分则设计了基于对的损失函数：
$$
\begin{align}
\min_{\mathbf{B}, \theta_x, \theta_y} \mathcal{J} &= -\sum_{i,j=1}^n (S_{ij}\Theta_{ij} - \log(1+e^{\Theta_{ij}})) \\
& + \gamma(\|\mathbf{B} - \mathbf{F}\|_F^2 + \|\mathbf{B} - \mathbf{G}\|_F^2) \\
& + \eta(\|\mathbf{F1}\|_F^2 + \|\mathbf{G1}\|_F^2) \qquad \qquad \qquad \qquad  (2)\\
s.t. \quad & \mathbf{B} \in \{-1,+1\}^{c\times n}
\end{align}
$$
其中 $$x, y$$ 分别表示图片和文本模态，$$\theta$$ 表示网络参数，$$c$$ 是哈希码长度，$$B$$ 表示图片和文本的哈希码，$$F$$ 和 $$G$$ 分别是图片网络和文本网络输出的实值哈希码，$$\Theta = \frac 12 F_{*i}^TG_{*j}$$ 表示第 $$i$$ 个图像与第 $$j$$ 个文本的点积，$$\mathbf{1}$$ 表示全 1 向量。

每次更新时固定一些参数，更新另一些参数，交替更新，其他细节可以参考论文的[详细分析](https://wendellgul.github.io/research%20note/2018/08/12/Deep-Cross-Modal-Hashing/)。

#### DVSH (SIGKDD 2016)

论文：[Deep Visual-Semantic Hashing for Cross-Modal Retrieval][2]。DVSH主要针对文本模态是句子的跨模态检索，是清华大学软件学院龙明盛老师组的工作。与DCMH不同的是，DVSH使用LSTM来提取句子特征，同时还增加了**图像语义融合网络**，该网络将文本与图片特征结合，学习一个融合的哈希码，然后再让各自模态的哈希网络生成的哈希码与融合的哈希码尽可能接近。其他细节可以参考论文的[详细分析](https://wendellgul.github.io/research%20note/2018/09/11/Deep-Visual-Semantic-Hashing-for-Cross-Modal-Retrieval/)。

<center><img src="https://wendell-1251760226.cos.ap-beijing.myqcloud.com/2019-05-09-015022.jpg" /></center>
#### PRDH (AAAI 2017)

论文：[Pairwise Relationship Guided Deep Hashing for Cross-Modal Retrieval][3]。PRDH就是对DCMH的简单扩展，通过基于对的损失函数来学习哈希码，除了DCMH中存在的跨模态间损失和量化损失外，PRDH设计了**同模态间损失**和**去相关损失**，其中去相关损失是为了让哈希码的每一位提供的信息最大，是作用在哈希码每一位上的损失。其他细节的详细分析看[这里](https://wendellgul.github.io/research%20note/2018/09/13/Pairwise-Relationship-Guided-Deep-Hashing-for-Cross-Modal-Retrieval/)。

<center><img src="https://wendell-1251760226.cos.ap-beijing.myqcloud.com/2019-05-09-030345.jpg"/></center>
#### CHN (BMVC 2017)

论文：[Correlation Hashing Network for Efficient Cross-Modal Retrieval][4]。CHN 也是清华大学软件学院龙明盛老师组的工作，该方法沿用了 DCMH 的网络结构，也是使用 CNN 提取图片特征，MLP 提取文本特征，该方法也是针对 tag 型文本和图片的跨模态检索，主要的部分是提出了两个损失：**余弦最大间距损失** 和 **量化最大间距损失**。与 DCMH 等不同的是，CHN 没有使用点积作为汉明距离的替代，而是使用余弦距离作为更有效的汉明距离的代替。

<center><img src="https://wendell-1251760226.cos.ap-beijing.myqcloud.com/2019-05-09-030418.jpg" /></center>
其中余弦最大间距损失定义如下：

$$
L=\sum_{s_{i j} \in \mathcal{S}} \max \left(0, \delta-s_{i j} \frac{\left\langle\boldsymbol{u}_{i}, \boldsymbol{v}_{j}\right\rangle}{\left\|\boldsymbol{u}_{i}\right\|\left\|\boldsymbol{v}_{j}\right\|}\right)^{2}
$$

式中，$$\frac{\left\langle\boldsymbol{u}_{i}, \boldsymbol{v}_{j}\right\rangle}{\left\Vert \boldsymbol{u}_{i}\right\Vert \left\Vert\boldsymbol{v}_{j}\right\Vert}= \cos(\boldsymbol{u}_i, \boldsymbol{v}_j) $$，$$s_{ij} = 1$$ 表示图像表示 $$\boldsymbol{u}_i$$ 和文本表示 $$\boldsymbol{v}_j$$ 相似，否则 $$s_{ij} = -1$$，$$0 < \delta \le 1$$ 表示间距。 量化最大间距损失定义如下：

$$
Q=\sum_{i=1}^{n_{x}} \max \left(0, \delta-\frac{\left\langle\left|\boldsymbol{u}_{i}\right|, \mathbf{1}\right\rangle}{\left\|\boldsymbol{u}_{i}\right\|\|\mathbf{1}\|}\right)+\sum_{i=1}^{n_{y}} \max \left(0, \delta-\frac{\left\langle\left|\boldsymbol{v}_{i}\right| \boldsymbol{1}\right\rangle}{\left\|\boldsymbol{v}_{i}\right\|\|\mathbf{1}\|}\right)
$$

量化间距损失是为了让 $$\vert \boldsymbol{u}_i\vert$$ 和 $$\vert\boldsymbol{v}_j\vert$$ 与向量 $$\mathbf{1}$$ 更加接近。

#### CDQ (AAAI 2017)

论文：[Collective Deep Quantization for Efficient Cross-Modal Retrieval][5]。CDQ 是清华大学软件学院龙明盛老师组的工作，将量化乘积的思想引入到了跨模态哈希中，该方法针对的也是 tag 型文本和图片的跨模态检索。

<center><img src="https://wendell-1251760226.cos.ap-beijing.myqcloud.com/2019-05-09-030456.jpg" /></center>
CDQ 是在 CHN 网络结构上做的改变，没有直接通过网络最后的 FC 层生成全部的哈希码

#### THN (AAAI 2017)

论文：[Transitive Hashing Network for Heterogeneous Multimedia Retrieval][6]。THN 是清华大学软件学院龙明盛老师组的工作，

#### TVDB (ICCV 2017)

论文：Deep Binaries: Encoding Semantic-Rich Cues for Efficient Textual-Visual Cross Retrieval

#### CMDVH (CVPR 2017)

论文：Cross-Modal Deep Variational Hashing

#### ACMR (ACM MM 2017)

论文：Adversarial Cross-Modal Retrieval

#### SSAH (CVPR 2018)

论文：Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval

#### AttenDAH (ECCV 2018)

论文：Attention-aware Deep Adversarial Hashing for Cross-Modal Retrieval

#### CMHH (ECCV 2018)

论文：Cross-Modal Hamming Hashing

#### DDCMH (AAAI 2018)

论文：Dual Deep Neural Networks Cross-Modal Hashing

#### MCSCH (ACM MM 2018)

论文：Multi-Scale Correlation for Sequential Cross-modal Hashing Learning

#### DDSRH (ICMR 2018)

论文：Cross-Modal Retrieval Using Deep De-correlated Subspace Ranking Hashing

#### MCTD (ICMR 2018)

论文：Multi-view Collective Tensor Decomposition for Cross-modal Hashing

### 图像检索与深度哈希





[1]: http://openaccess.thecvf.com/content_cvpr_2017/papers/Jiang_Deep_Cross-Modal_Hashing_CVPR_2017_paper.pdf
[2]: http://delivery.acm.org/10.1145/2940000/2939812/p1445-cao.pdf?ip=159.226.95.66&id=2939812&acc=CHORUS&key=33E289E220520BFB%2E99E4F0382D256DD3%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1553070762_4ff86dba9a7d4ea9418a052d8d0271a7
[3]: https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14326
[4]: https://arxiv.org/pdf/1602.06697.pdf
[5]: https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14499
[6]: https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14559
